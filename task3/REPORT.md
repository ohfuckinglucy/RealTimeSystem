# 1. Теоретические основы IPC в Linux

### IPC (Inter-Process Communication)

Межпроцессное взаимодействие — набор механизмов, обеспечивающих обмен данными между процессами.
Наиболее распространённые IPC-механизмы Linux:

| Механизм                 | Скорость      | ОС-поддержка | Структурированность   | Асинхронность           |
| ------------------------ | ------------- | ------------ | --------------------- | ----------------------- |
| **UNIX-сокеты**          | средняя       | везде        | байтовый поток        | да                      |
| **epoll**                | высокая       | Linux        | нет                   | да (массовая обработка) |
| **eventfd**              | высокая       | Linux        | 64-bit счётчик        | да                      |
| **POSIX Message Queues** | средняя       | POSIX        | сообщения + приоритет | да                      |
| **Shared Memory**        | максимальная  | везде        | общие структуры       | нет (нужны семафоры)    |
| **readv/writev**         | ускорение I/O | везде        | буфера массива        | нет                     |

---

# 2. Описание реализованных компонентов

---

## 2.1. epoll_server.c — асинхронный сервер

Демонстрирует работу:

* **UNIX-domain сокетов** (локальные, быстрые, надёжные)
* **epoll** — механизм массовой обработки событий (многопоточность не требуется)
* **eventfd** — механизм сигналов/уведомлений между потоками или подсистемами

Особенности:

* сервер создаёт `/tmp/epoll_server.sock`
* добавляет в epoll:

  * слушающий сокет
  * eventfd
  * клиентские сокеты, появляющиеся при `accept4`
* работает в режиме **edge-triggered (EPOLLET)**

Как протестировать:

```
./bin/epoll_server
socat - UNIX-CONNECT:/tmp/epoll_server.sock
```

Создать внутреннее событие:

```
echo 1 > /proc/<PID>/fd/<eventfd>
```

---

## 2.2. iov_demo.c — пример readv/writev (Scatter/Gather I/O)

Демонстрация:

* записи сразу нескольких буферов за один вызов `writev`
* чтения нескольких буферов `readv`
* отсутствие лишних копирований памяти
* небольшая структура `message_t` передаётся по pipe

Использование показывает оптимизацию ввода-вывода без объединения разных частей данных в один буфер.

---

## 2.3. posix_mq_server.c / posix_mq_client.c — POSIX Message Queues

Показано:

* обмен сообщениями «клиент ⇄ сервер»
* приоритеты сообщений (HIGH и NORMAL)
* преобразование текста в верхний регистр на стороне сервера
* использование очередей:

  * `/mq_server_ex`
  * `/mq_client_ex`

Клиент отправляет 3 сообщения с разными приоритетами, сервер читает их в порядке приоритетов.

---

## 2.4. Shared Memory + Semaphores (shm_common.h, shm_consumer.c и producer)

Используется:

* POSIX shared memory: `shm_open / mmap`
* два семафора:

  * `/sem_producer_ex`
  * `/sem_consumer_ex`
* кольцевой буфер `shared_data_t.buffer[10]`
* синхронизация через семафоры

Работа потребителя:

* ждёт уведомления семафора
* читает числовые значения из буфера
* корректно обрабатывает SIGINT + SIGTERM

---

# 3. Выводы

1. **epoll + UNIX-сокеты + eventfd**
   — оптимальный и масштабируемый способ построения высокопроизводительных серверов.

2. **POSIX Message Queues**
   — удобный механизм с приоритетами, однако менее производительный, чем shared memory.

3. **readv/writev**
   — упрощают работу с фрагментированными данными и уменьшают количество системных вызовов.

4. **Shared Memory + Semaphores**
   — самый быстрый механизм IPC, но требует ручной синхронизации и аккуратного дизайна.

5. Механизмы IPC отличаются:

   * скоростью
   * моделью взаимодействия
   * надёжностью
   * удобством применения
     Поэтому выбор IPC зависит от конкретной задачи.
---

# **Контрольные вопросы**

 
### 1. **POSIX MQ vs. UNIX Sockets**

* **Очереди сообщений (POSIX MQ)** лучше использовать, когда требуется **асинхронная и упорядоченная передача сообщений между процессами на одной машине**, с возможностью задания приоритетов.
  **Сценарий:** два процесса на одной системе обмениваются заданиями на выполнение: продюсер кладёт задачи в очередь сообщений, потребитель читает их по приоритету.

* **UNIX сокеты** подходят, когда нужна **связь между процессами с потоком данных**, особенно если требуется передача больших объёмов или бинарных данных, а также возможность сетевого взаимодействия (локально или через TCP/IP).
  **Сценарий:** локальный клиент и сервер обмениваются JSON-сообщениями через UNIX сокет `/tmp/app.sock`.

---

### 2. **Edge-Triggered (ET) vs. Level-Triggered (LT) в `epoll`**

* **Level-Triggered (LT, поведение по умолчанию)**: `epoll` уведомляет процесс, пока **дескриптор готов для чтения/записи**. Даже если вы не прочитали все данные, событие снова сработает.
* **Edge-Triggered (ET, `EPOLLET`)**: событие срабатывает **только при изменении состояния** дескриптора (например, появились новые данные). Если вы не прочитали все данные за один раз, следующие события **не придут**, пока не появится что-то новое.

**Сложнее:** ET сложнее, потому что нужно читать данные **до конца**, иначе можно потерять события. Типичные ошибки:

* Не считать все доступные данные в цикле `read()`, думая, что новое событие придёт автоматически.
* Использовать блокирующие `read()`, что может привести к зависанию программы.

---

### 3. **Семафоры vs. Мьютексы для разных процессов**

* **Можно ли использовать `pthread_mutex_t` для синхронизации между разными процессами?**
  Да, можно, **если задать специальные атрибуты мьютекса**, чтобы он был **межпроцессным (process-shared)**.

Пример:

```c
pthread_mutexattr_t attr;
pthread_mutexattr_init(&attr);
pthread_mutexattr_setpshared(&attr, PTHREAD_PROCESS_SHARED);
pthread_mutex_init(&mutex, &attr);
```

* Без `PTHREAD_PROCESS_SHARED` мьютекс будет работать только **в пределах одного процесса** (для потоков).
* Семафоры (`sem_t`) по умолчанию могут быть **именованными или разделяемыми через память**, что упрощает использование между процессами.

---

### 4. **Копирование данных ядром**

Рассмотрим три механизма передачи данных между процессами:

1. **POSIX MQ (Message Queue)** — ядро копирует данные **из пользователя в ядро и обратно**. При каждом `send` и `receive` происходит минимум 2 копирования.
2. **UNIX Sockets** — тоже копирование через ядро: **user → kernel → user**, аналогично MQ.
3. **Shared Memory (SHM)** — данные находятся **прямо в разделяемой области памяти**, ядро участвует только в управлении доступом (семафоры/мьютексы), **копирования нет**.

**Порядок по возрастанию количества копирований:**

```
Shared Memory < POSIX MQ ≈ UNIX Sockets
```

**Объяснение:** SHM исключает лишнее копирование, а MQ и сокеты всегда проходят через буфер ядра.